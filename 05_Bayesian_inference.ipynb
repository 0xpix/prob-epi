{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bayesian inference\n",
        "\n",
        "Bayesian inference is the process of deducing properties of a probability distribution from data using Bayesâ€™ theorem. It incorporates the idea that probability should include a measure of **belief** about a prediction or outcome.\n",
        "\n",
        "\n",
        "Bayesian inference is a method of statistical inference in which probability is used to *update beliefs* about model's parameters based on available *evidence or data*.\n",
        "\n",
        "**Group task 1:**\n",
        "\n",
        "To better understand the role of prior <font color='purple'>`beliefs and subjective probability`</font>, discuss with your neighbour the following questions:\n",
        "\n",
        "\n",
        "- What is the probability that it will rain tomorrow?\n",
        "- What is the probability that the next president will be a woman?\n",
        "- What is the probability that aliens built the pyramids?\n",
        "\n",
        "How do these questions compare to the probability that a die will roll a 6?\n",
        "\n",
        "Such questions, unlike the die, cannot be answered by \"long-run\" probability, i.e., probability obtained from multiple repeated runs of the same experiment. A certain degree of <font color='purple'>`belief`</font> ðŸ’­ is involved.\n",
        "\n",
        "<font color='purple'>`Priors`</font> ðŸ’­ and \"subjective\" probability are foundational for Bayesian inference!\n",
        "\n",
        "## Bayes theorem\n",
        "\n",
        "_Note: Conditional probability is the axis on which Bayesian statistics turns! If you skipped this section in the first part of the practical and need a refresher on joint and conditional probabilities, go back to that bit of the prac to make sure you are ready for the phenomenon of Bayes' Theorem!_\n",
        "\n",
        "From the joint and conditional formulae, we can relate two conditional probabilities:\n",
        "\n",
        "\\begin{equation}\n",
        "p(B \\mid A) = \\frac{p(A \\mid B) p(B)}{p(A)}\n",
        "\\end{equation}\n",
        "\n",
        "This is the same equation you have already explored...\n",
        "\n",
        "...but with a slightly different interpretation.\n",
        "\n",
        "**The famous Bayes Theorem!** ðŸ˜»\n",
        "\n",
        "Why is it so famous? Well let's understand what it means and gives us first!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prior, likelihood, posterior\n",
        "\n",
        "Bayes' Theorem is commonly seen in machine learning and other models using <font color='red'>`data or evidence`</font> $\\mathcal{D}$ and <font color='purple'>`parameters`</font> $\\theta$ as:\n",
        "\n",
        "\\begin{equation}\n",
        "p(\\theta \\mid \\mathcal{D}) = \\frac{p(\\mathcal{D} \\mid \\theta)p(\\mathcal{\\theta})}{p(D)}\n",
        "\\end{equation}\n",
        "\n",
        "* The denominator $p(\\mathcal{D})$ is often called a *normaliser* or <font color='red'>`evidence`</font> .\n",
        "* $p(\\mathcal{\\theta})$ is the <font color='purple'>`prior`</font> .\n",
        "* $p(\\mathcal{D} \\mid \\theta)$ is the <font color='teal'>`likelihood`</font> .\n",
        "* and $p(\\theta \\mid \\mathcal{D})$ is the <font color='pink'>`posterior`</font>.\n",
        "\n",
        "For this reason, you will often see Bayes rule summarised as \"<font color='pink'>`posterior`</font> $\\propto$ <font color='purple'>`prior`</font> $\\times$ <font color='teal'>`likelihood`</font> \", which ignores the denominator since it is a constant (independent of $\\mathcal{D}$). This posterior summarises our belief state about the possible values of $\\theta$.\n",
        "\n",
        "**Group task 2:** _Switch $A$ and $B$ in $\n",
        "p(B \\mid A) = \\frac{p(A \\mid B)p(B)}{p(A)}$ and label what is the posterior, likelihood, prior and evidence in the new equation. Explain what each means to each other a few times to be sure you know what goes where! _Often the literature will refer to these 'labels' and require you know them already, so while they might seem arbitrary this memorisation task goes a long way to making your life easier in practice!_\n",
        "\n",
        "## Diagnosing cause of headache\n",
        "\n",
        "Imagine a situation where you need make a decision concerning your health. You have a headache, and can choose between two doctors:\n",
        "\n",
        "**Doctor 1:**\n",
        "- Has a mental model for the cause of pain.\n",
        "- Performs tests.\n",
        "\n",
        "**Doctor 2:**\n",
        "- Has a mental model for the cause of pain.\n",
        "- Has access to the patient's chronic history.\n",
        "- Performs tests.\n",
        "\n",
        "Which doctor do you choose? Can you make sense of which parts are the <font color='red'>`data`</font>, <font color='teal'>`likelihood`</font> and <font color='purple'>`prior`</font> in this scenario?\n",
        "\n",
        "Inference without priors is like a doctor who does not know the patient's history!\n",
        "\n",
        "## Diagnosing COVID-19\n",
        "\n",
        "**Group task 3:** We know that the probability of having fever this time of the year is 10%, the probability of having COVID is 7%, and among all people who have COVID, 70% of them have fever.\n",
        "\n",
        "If you're a doctor, you don't know whether someone has COVID until you test them, but they may present with a high temperature and you want to reason whether to isolate them on that basis! So you are interested in knowing the chance that someone has COVID given they have a high temperature.\n",
        "\n",
        "Find the probability that a patient has COVID given they have high temperature (fever).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "import numpyro.distributions as dist\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How can we perform Bayesin inference?\n",
        "\n",
        "### What does it take?\n",
        "- <font color='red'>`Data`</font>\n",
        "- A generative model (how does the conditional <font color='teal'>`likelihood`</font> come about?)\n",
        "- Our <font color='purple'>`beliefs`</font> before seeing the data.\n",
        "\n",
        "### What does it make?\n",
        "- The values of parameters that could give rise to the observed data **in the form of a distribution**.\n",
        "\n",
        "\n",
        "### How can we perform it?\n",
        "\n",
        "- **Analytically**\n",
        "        \n",
        "     Solving the maths! This is an elegant approach. However, it is rarely available in real life.\n",
        "\n",
        "- **Numerically**\n",
        "\n",
        "    - Rather than deriving a posterior distribution in the closed form, we can use computational tools to **sample** from the posterior. The obtained samples describe the distributions of parameters.\n",
        "    \n",
        "    - We achieve this by exploring the space of parameters to find the most probable combinations of parameters.\n",
        "    \n",
        "    - Further we treat the obtained sampled as new data, to extract information about parameters, such as mean, credible interval or other statistics.\n",
        "\n",
        "### Numerical methods\n",
        "\n",
        "- Markov Chain Monte Carlo (MCMC) family of algorithm, e.g.,\n",
        "  * Metropolis-Hastings\n",
        "  * Gibbs\n",
        "  * Hamiltonian Monte Carlo (HMC)\n",
        "  * No-U-Turn sampler (NUTS)\n",
        "  * further variants such as SGHMC, LDHMC, etc\n",
        "- Variational Bayes\n",
        "- Approximate Bayesian Computation (ABC)\n",
        "- Particle filters\n",
        "- Laplace approximation\n",
        "\n",
        "More on this later! First, let's discuss some analytics and point estimates.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analytical Bayesian inference: Point estimates for Bernoulli-beta coin flips - [<font color='orange'>`Intermediate`</font>]\n",
        "\n",
        "### Point estimates\n",
        "\n",
        "To illustrate the use of Bayes' Theorem further, let's explore the coin flip example that you saw with the Bernoulli distribution to try to figure out whether a coin is weighted or not. ðŸ’°\n",
        "\n",
        "We really just want one answer out of this problem -- the probability that a coin will give us a heads (since we know that the probability of tails is just $1- p(heads)$). This is a <font color='green'>`point estimate`</font>: one answer out of a range.\n",
        "\n",
        "In machine learning, we are often interested in estimating parameters $\\theta$ that best allow us to describe our data. Generally this leads to solving some optimisation problem for a loss function $\\mathcal{L}$, i.e.\n",
        "\\begin{equation*}\n",
        "\\hat{\\theta} = {\\arg \\min}_\\theta \\mathcal{L}(\\theta)\n",
        "\\end{equation*}\n",
        "\n",
        "This gives us a <font color='green'>`point estimate`</font> $\\hat{\\theta}$.\n",
        "\n",
        "**Group task 6**: Does a point estimate tell us anything about our uncertainty or the distribution from which we draw the estimate? Discuss the difference between <font color='green'>`point estimates`</font> and estimating a *distribution*."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
